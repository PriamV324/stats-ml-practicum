---
title: "Statistics and ML"
author: "Priam Vyas"
date: "2023-01-24"
output:
  pdf_document: default
  html_document: default
subtitle: MSSP Practicum Discussion
---   

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Instructions

**Fork** the [`carvalho/stats-ml-practicum`](https://github.com/carvalho/stats-ml-practicum) repository at GitHub, and
**create a new branch with your BU login** to store your changes to the document.
Start by changing the `author`in the YAML header of the document to state **your name**.

Below we run some analyses and ask questions about them. As you run the code and
interpret the results within your group, write your answers to the questions following the analyses, but:

> You should submit your work as a **pull request** to the original repository!


## Introduction

In this project we study **tree canopy cover** as it varies with the
**relative distance** to a tree line boundary in urban forests. The dataset in
`stats-ml-canopy.RData` has three variables: `location` for the urban forest
where the canopy cover was observed, `distance` for the relative distance &mdash;
zero is inside the forest and one is outside (city) &mdash; and `cover` for the
canopy cover.

```{r}
load("stats-ml-canopy.RData")
(canopy <- as_tibble(canopy))

idx <- order(canopy$distance) # for plots below
ggplot(canopy, aes(distance, cover)) + geom_point(color = "gray")
```

As can be seen, there is a clear pattern here: the canopy cover starts high,
closer to 100% when inside the forest, but as the tree line recedes into the
city, the canopy cover approaches zero.

We are interested in two main tasks:

- **Understanding** this relationship more explicitly;
- **Predicting** the canopy cover at the assumed tree line boundary when
`distance` is 0.5.

To this end, we explore four approaches below.

## Statistics 1: Linear Fit

```{r stats1}
m <- glm(cover ~ distance, data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")

#residual plot
ggplot(m, aes(x = .fitted, y = .resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)
```

Questions and tasks:

- Comment on the fit, plot residuals and comment on them.
#The fit is not a  good  since its trying to fit a logistic function to a linear data set. The residuals are low in the upper and lower half of the distance since the logistic model only has 2 options to choose from and for the points that lie in the central region, the residuals are further spread apart.

- Comment on the prediction; does it seem reasonable?
#The prediction isn't too good as the model is a logistic model that is trying to predict values that are on the continuous scale.


## ML 1: LOESS

```{r ml1}
m <- loess(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check the definition of the `loess` function; how does it differ from the previous approach?
#The loess function is a polynomial function that fits one or more numerical predictors while the previous approaches were no polynomial. 

- Comment on the fit; does it seem reasonable?
#The fit still doesn't seem to be very good as there are still a lot of points in the central region that lie far from the estimated line.

- Comment on the prediction, including the SE.
#The SE of the model is close to 0 that means that model is predicting the values descently.


## ML 2: Random Forest

```{r ml2,message=FALSE}
library(randomForest)
m <- randomForest(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], predict(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check what `randomForest` does; what is **keyword** here?
#randomForest is a classification and regression. The keyword is random as it takes multiple values randomly to make the multiple trees.

- Comment on the fit; how does it differ from the previous fits?
#The fit is decent fit but isn't as smooth as the previous ones. 

- Comment on the prediction; how would you obtain a measure of uncertainty?
#The prediction is decent. 

## Statistics 2: Cubic Fit

```{r stats2}
m <- glm(cover ~ poly(distance, 3), data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")

#residual plot
ggplot(m, aes(x = .fitted, y = .resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)
```

Questions and tasks:

- Comment on the fit and compare it to the first model; plot and check residuals.
#The fit is good with 0.5 value. The SE of the fit is also low telling us its a decent model for the data.

- Comment on the prediction and compare it to previous results.
#The residual scale is also low which tells us the model has good prediction power. It is lower than all the previous models.

- How would you know that a cubic fit is good enough?
#The residual scale is the lowest when compared to the other models created.


## Discussion

Let's try to connect all lessons learned from your work and the discussions.
Elaborate more on the following questions:

- How would you know that the predictions are *reliable*?
#Since the standard error is close to 0 we can say that the predictions are reliable.

- How would you test that the cover is exactly 50% at the boundary (`distance` = 0.5)? Which approaches would make the test easier to perform?
#We can tests it by using the divide by 4 rule and verify that the cover is exactly 50%

- How would you incorporate `location` in your analyses? How would you know that
it is meaningful to use it?
#If the distance is 0 then we know that the location is rural and if its 1 it a urban location. We can change these to binary and see a contrast between the 2 locations.

